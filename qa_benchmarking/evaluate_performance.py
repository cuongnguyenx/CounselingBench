import pandas as pd
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
import os
import random
import scipy.stats as st
import numpy as np
from collections import Counter
from ast import literal_eval
from itertools import chain
from collections import Counter
import sentence_transformers

def load_output(file):
    """Loads the output file generated by QA benchmarking scripts

    Args:
        file (str): Name of the output file, corresponding to a LLM benchmark (.csv)
    Returns:
        df: Dataframe containing benchmark data and corresponding LLM answers 
    """
    df = pd.read_csv(file)
    df = df[~pd.isna(df['Question'])]
    df = df[df['Question'].str.len() >= 5]
    df_gold = pd.read_csv('../data/mct_combined_v3.csv')

    df['label'] = df_gold['question_category']

    # dfx = pd.read_csv('../data/tiebreaking_needed.csv')
    # df = df[~df['Question'].isin(dfx['Question_x'])]
    # print(df.shape[0])
    # print(df)
    return df

def parse_answers(answer, filename):
    if answer is None or isinstance(answer, float):
        return 'Z'
    
    if '_sc_' in filename:
        parsed_list = list(chain.from_iterable(literal_eval(answer)))
        answer = Counter(parsed_list).most_common(1)[0][0]

    answer = str.lower(answer)
    if 'few-shot-cot' in filename:
        answer = answer[answer.rfind("\n") + 1:]
        # print(answer)
        # print('@@@@@@@')
    if 'the correct answer is (a)' in answer and 'the correct answer is (b)' in answer:
        return 'Z'
    if 'the correct answer is (a)' in answer or 'correct answer: (a)' in answer or 'the correct answer here is (a)' in answer or 'answer: (a)' in answer or 'the appropriate answer is (a)' in answer or '(a):' in answer or 'the answer here is (a)' in answer:
        return 'A'
    elif 'the correct answer is (b)' in answer or 'correct answer: (b)' in answer or 'the correct answer here is (b)' in answer or 'answer: (b)' in answer or 'the appropriate answer is (b)' in answer or '(b):' in answer or 'the answer here is (b)' in answer:
        return 'B'
    elif 'the correct answer is (c)' in answer or 'correct answer: (c)' in answer or 'the correct answer here is (c)' in answer or 'answer: (c)' in answer or 'the appropriate answer is (c)' in answer or '(c):' in answer or 'the answer here is (c)' in answer:
        return 'C'
    elif 'the correct answer is (d)' in answer or 'correct answer: (d)' in answer or 'the correct answer here is (d)' in answer or 'answer: (d)' in answer or 'the appropriate answer is (d)' in answer or '(d):' in answer or 'the answer here is (d)' in answer:
        return 'D'
    
    a_loc = answer.find('(a)')
    b_loc = answer.find('(b)')
    c_loc = answer.find('(c)')
    d_loc = answer.find('(d)')
    if a_loc == -1:
        a_loc = 1e+9
    if b_loc == -1:
        b_loc = 1e+9
    if c_loc == -1:
        c_loc = 1e+9
    if d_loc == -1:
        d_loc = 1e+9
        
    if answer is None or pd.isna(answer):
        return "Z"
    if 'answer: A' in answer or 'a:' in answer or 'correct answer is a' in answer or 'correct answer: a' in answer or answer == ' a' or answer == 'a' or answer == ' 1' or answer == '1' or answer == ' 0' or answer == '0':
        return 'A'
    elif 'answer: b' in answer or 'b:' in answer or 'correct answer is b' in answer or 'correct answer: b' in answer  or answer == ' b' or answer == 'b' or answer == ' 2' or answer == '2':
        return 'B'
    elif 'answer: c' in answer or 'c:' in answer or 'correct answer is c' in answer or 'correct answer: c' in answer  or answer == ' c' or answer == 'c' or answer == ' 3' or answer == '3':
        return 'C'
    elif 'answer: d' in answer or 'd:' in answer or 'correct answer is d' in answer or 'correct answer: d' in answer  or answer == ' d' or answer == 'd' or answer == ' 4' or answer == '4':
        return 'D'
    
    if min(a_loc, b_loc, c_loc, d_loc) == a_loc:
        return 'A'
    if min(a_loc, b_loc, c_loc, d_loc) == b_loc:
        return 'B'
    if min(a_loc, b_loc, c_loc, d_loc) == c_loc:
        return 'C'
    if min(a_loc, b_loc, c_loc, d_loc) == d_loc:
        return 'D'
    
    return 'Z'
    
    
    
def map_output(arr):
    """Transforms an array of alphabetic answers into their corresponding numeric mapping

    Args:
        arr (list): Array of alphabetic answers, including 'A', 'B', 'C', 'D', 'Z'
    Returns:
        new_arr (list): Array containing numeric mappings of alphabetic answers, following {'A': 0, 'B': 1, 'C': 2, 'D': 3, 'Z': 4}
    """
    # Assuming arr is an array consisting of 'A', 'B', 'C', and 'D'
    new_arr = []
    for elem in arr:
        if elem == 'A':
            new_arr.append(0)
        elif elem == 'B':
            new_arr.append(1)
        elif elem == 'C':
            new_arr.append(2)
        elif elem == 'D':
            new_arr.append(3)
        else:
            new_arr.append(4)
    return new_arr

SEED = 125
constant = 1e9
random.seed(SEED)
def bootstrapped_metrics(num_bootstrap, df):
    b_accuracies = []
    b_f1 = []
    b_precision = []
    b_recall = []
    for tries in range(num_bootstrap):
        seed = int(random.random() * constant)
        dff = df.sample(100, random_state = seed, replace=True)
        metrics_dict = generate_metrics(dff)
        b_accuracies.append(metrics_dict['Accuracy'])
        b_f1.append(metrics_dict['F1'])
        b_precision.append(metrics_dict['Precision'])
        b_recall.append(metrics_dict['Recall'])
    ci_acc = st.t.interval(confidence=0.99, df=len(b_accuracies)-1, 
              loc=np.mean(b_accuracies), 
              scale=st.sem(b_accuracies))
    ci_f1 = st.t.interval(confidence=0.99, df=len(b_f1)-1, 
              loc=np.mean(b_f1), 
              scale=st.sem(b_f1))
    ci_precision = st.t.interval(confidence=0.99, df=len(b_precision)-1, 
              loc=np.mean(b_precision), 
              scale=st.sem(b_precision))
    ci_recall = st.t.interval(confidence=0.99, df=len(b_recall)-1, 
              loc=np.mean(b_recall), 
              scale=st.sem(b_recall))
    return {'Accuracy': ci_acc, 'F1': ci_f1, 'Precision': ci_precision, 'Recall': ci_recall}
        
        
def generate_metrics(df):
    # Accuracy
    correct_answers = df['correct_answer_letter']
    pred_answers = df['parsed_pred']
    p_ans = map_output(pred_answers)
    c_ans = map_output(correct_answers)
    # print(Counter(p_ans))
    # print(Counter(c_ans))
    acc_score = accuracy_score(c_ans, p_ans)
    f1 = f1_score(c_ans, p_ans, average='macro', zero_division=0)
    precision = precision_score(c_ans, p_ans, average='macro', zero_division=0)
    recall = recall_score(c_ans, p_ans, average='macro', zero_division=0)
    if np.isnan(f1):
        f1 = 0.0
    if np.isnan(precision):
        f1 = 0.0
    if np.isnan(recall):
        f1 = 0.0
    return {'Accuracy': acc_score, 'F1': f1, 'Precision': precision, 'Recall': recall}


def write_metrics(metrics, labels, setting):
    with open(f'../results/metrics/{setting}.txt', 'w') as f:
        for metric, label in zip(metrics, labels):
            f.write("\n")
            f.write("###############################")
            f.write("\n")
            f.write(label)
            f.write("\n\n")
            for key in metric.keys():
                f.write(key)
                f.write("\n")
                f.write(str(metric[key]))
                f.write("\n")
        print(f"Saving to ../results/metrics/{setting}.txt")
        f.close() 
    return

def generate_metrics_csv(metrics, labels, setting):
    curr_dict = dict([])
    curr_dict['setting'] = setting
    for metric, label in zip(metrics, labels):
        for key in metric.keys():
            new_key = str(key) + "_" + str(label)
            curr_dict[new_key] = (metric[key][0] + metric[key][1]) / 2
    return curr_dict

if __name__ == "__main__":
    metrics_combined = []
    for file in os.listdir('../results/outputs'):
        if os.path.isdir(os.path.join('../results/outputs', file)) or 'ipynb' in file or 'error' in file:
            continue
        print(file)
        try:
            df = load_output(os.path.join('../results/outputs', file))
        except:
            continue
        if '_sc_' not in file:
            arr = []
            for x in df['raw_pred']:
                letter = parse_answers(x, file)
                arr.append(letter)
            df['parsed_pred'] = arr
            # df['parsed_pred'] = [parse_answers(x, file) for x in df['raw_pred']
        else:
            df['parsed_pred'] = [parse_answers(x, file) for x in df['consistency']]
        df.to_csv(os.path.join('../results/outputs', file), index=False)
        labels = list(dict(Counter(df['label'])).keys())
        labels = ['Overall'] + labels
        print(labels)
        # labels.remove('?')
        try:
            labels.remove(np.nan)
        except:
            pass
        try:
            labels.remove('counseling')
        except:
            pass
        try:
            metrics = []
            # Overall
            for label in labels:
                if label == 'Overall':
                    metric = bootstrapped_metrics(100, df)
                else:
                    df_curr = df[df['label'] == label]
                    metric = bootstrapped_metrics(100, df_curr)
                metrics.append(metric)
            # print(metrics)
            write_metrics(metrics, labels, file.replace(".csv", ""))
            metrics_dict = generate_metrics_csv(metrics, labels, file.replace(".csv", ""))
            metrics_combined.append(metrics_dict)
        except Exception as e:
            print('ERROR!')
            print(e)
            continue
    pd.DataFrame(metrics_combined).to_csv('./CounselingQA_metrics_combined.csv', index=False)
    pd.DataFrame(metrics_combined).to_csv('../statistical_analysis/CounselingQA_metrics_combined.csv', index=False)