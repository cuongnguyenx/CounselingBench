{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'core counseling attributes',\n",
       " 'counseling',\n",
       " 'counseling skills and interventions',\n",
       " 'intake, assessment, and diagnosis',\n",
       " nan,\n",
       " 'professional practice and ethics',\n",
       " 'treatment planning'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dx = pd.read_csv('../data/mct_combined_v3.csv')\n",
    "set(dx['question_category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframe(file, ground_df):\n",
    "    df = pd.read_csv(file)\n",
    "    df = df[~pd.isna(df['Question'])]\n",
    "    question_categories = []\n",
    "    for row in df.iterrows():\n",
    "        try:\n",
    "            question_categories.append(ground_df[ground_df['Question'] == row[1]['Question']].iloc[0]['question_category'])\n",
    "        except:\n",
    "            question_categories.append('NA')\n",
    "    df['question_category'] = question_categories\n",
    "    df = df[df['question_category'] != \"NA\"]\n",
    "    print(df.shape[0])\n",
    "    return df\n",
    "\n",
    "def get_avgs_category(df, category):\n",
    "    dfx = df[df['question_category'] == category]\n",
    "    try:\n",
    "        more_ratio = dfx[dfx['case_label'].str.contains('more comprehensive')].shape[0] / dfx.shape[0]\n",
    "    except:\n",
    "        more_ratio = -1\n",
    "    return {'cosine_similarity': np.average(dfx['cosine_similarity']), 'bert_score': np.average(dfx['bert_score']), 'length_difference': np.average(dfx['length_difference']), 'length_prediction': np.average(dfx['length_prediction']), 'more_comprehensive': more_ratio}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalist_models = ['llama-2-7b-chat-hf', 'Llama-2-70b-hf', 'Llama-2-7b-hf', 'Llama-2-13b-hf', 'Llama-2-70b-hf', 'Llama-2-70b-hf', 'Meta-Llama-3-8B-Instruct', 'Meta-Llama-3-8B-Instruct', 'Meta-Llama-3-70B-Instruct', 'Llama-2-7b-hf', 'Llama-2-13b-chat-hf']\n",
    "medical_models = ['BioMedGPT-LM-7B', 'med42-70b', 'Asclepius-7B', 'medalpaca-13b', 'meditron-70b', 'ClinicalCamel-70B', 'Llama3-Med42-8B', 'Asclepius-Llama3-8B', 'Llama3-Med42-70B', 'meditron-7b', 'MentaLLaMA-chat-13B']\n",
    "generalist_models_l = ['Llama-2-70b-hf', 'Llama-2-70b-hf', 'Meta-Llama-3-8B-Instruct', 'Meta-Llama-3-70B-Instruct', 'Llama-2-13b-chat-hf']\n",
    "medical_models_l = ['meditron-70b', 'ClinicalCamel-70B', 'Llama3-Med42-8B', 'Llama3-Med42-70B', 'MentaLLaMA-chat-13B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['intake, assessment, and diagnosis', 'treatment planning', 'professional practice and ethics', 'counseling skills and interventions', 'core counseling attributes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1610\n",
      "Llama-2-13b-hf\n",
      "Need to redo\n",
      "1610\n",
      "med42-70b\n",
      "1610\n",
      "Llama-2-70b-chat-hf\n",
      "1610\n",
      "llama-2-7b-chat-hf\n",
      "1610\n",
      "Llama3-Med42-8B\n",
      "1610\n",
      "BioMedGPT-LM-7B\n",
      "1610\n",
      "Llama-2-7b-hf\n",
      "1610\n",
      "Llama-2-13b-chat-hf\n",
      "1610\n",
      "Llama3-Med42-70B\n",
      "1610\n",
      "Meta-Llama-3-70B-Instruct\n",
      "1610\n",
      "meditron-7b\n",
      "1610\n",
      "MentaLLaMA-chat-13B\n",
      "1610\n",
      "medalpaca-7b\n",
      "1610\n",
      "ClinicalCamel-70B\n",
      "1610\n",
      "Llama-2-70b-hf\n",
      "1610\n",
      "medalpaca-13b\n"
     ]
    }
   ],
   "source": [
    "model_dict = dict([])\n",
    "for file in os.listdir('../results/outputs_reasoning/'):\n",
    "    if 'ipynb' in file:\n",
    "        continue\n",
    "    df = load_dataframe(os.path.join('../results/outputs_reasoning/', file), dx)\n",
    "    model_name = file.replace('.csv', '')\n",
    "    print(model_name)\n",
    "    model_dict[model_name] = []\n",
    "    try:\n",
    "        for category in categories:\n",
    "            model_dict[model_name].append(get_avgs_category(df, category))\n",
    "    except:\n",
    "        print(\"Need to redo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################\n",
      "meditron-70b\n",
      "#####################################\n",
      "ClinicalCamel-70B\n",
      "intake, assessment, and diagnosis\n",
      "{'cosine_similarity': 0.28132107056116934, 'bert_score': 0.9956709447103146, 'length_difference': -129.9042207792208, 'length_prediction': 24.43831168831169, 'more_comprehensive': 0.06926406926406926}\n",
      "treatment planning\n",
      "{'cosine_similarity': 0.25357181585474514, 'bert_score': 0.9999999347634203, 'length_difference': -132.65846456692913, 'length_prediction': 20.561023622047244, 'more_comprehensive': 0.06299212598425197}\n",
      "professional practice and ethics\n",
      "{'cosine_similarity': 0.20087749107430378, 'bert_score': 0.9999999235837888, 'length_difference': -142.4981684981685, 'length_prediction': 17.57234432234432, 'more_comprehensive': 0.054945054945054944}\n",
      "counseling skills and interventions\n",
      "{'cosine_similarity': 0.30796388455883444, 'bert_score': 0.9999999456868899, 'length_difference': -139.9250418760469, 'length_prediction': 28.048157453936348, 'more_comprehensive': 0.06867671691792294}\n",
      "core counseling attributes\n",
      "{'cosine_similarity': 0.18141965086207443, 'bert_score': 0.9999999481698741, 'length_difference': -148.6413043478261, 'length_prediction': 12.66304347826087, 'more_comprehensive': 0.043478260869565216}\n",
      "#####################################\n",
      "Llama3-Med42-8B\n",
      "intake, assessment, and diagnosis\n",
      "{'cosine_similarity': 0.6890090020913712, 'bert_score': 0.9956709613531699, 'length_difference': -49.10010822510822, 'length_prediction': 105.24242424242425, 'more_comprehensive': 0.2922077922077922}\n",
      "treatment planning\n",
      "{'cosine_similarity': 0.6638003008982797, 'bert_score': 0.9999999671470462, 'length_difference': -42.98425196850393, 'length_prediction': 110.23523622047244, 'more_comprehensive': 0.3858267716535433}\n",
      "professional practice and ethics\n",
      "{'cosine_similarity': 0.6405918541681636, 'bert_score': 0.9999999495653006, 'length_difference': -75.40750915750915, 'length_prediction': 84.66300366300366, 'more_comprehensive': 0.25274725274725274}\n",
      "counseling skills and interventions\n",
      "{'cosine_similarity': 0.6540156143095026, 'bert_score': 0.9999999658546256, 'length_difference': -70.05904522613065, 'length_prediction': 97.9141541038526, 'more_comprehensive': 0.2914572864321608}\n",
      "core counseling attributes\n",
      "{'cosine_similarity': 0.5910191717355148, 'bert_score': 0.9999999196633048, 'length_difference': -91.58695652173913, 'length_prediction': 69.71739130434783, 'more_comprehensive': 0.13043478260869565}\n",
      "#####################################\n",
      "Llama3-Med42-70B\n",
      "intake, assessment, and diagnosis\n",
      "{'cosine_similarity': 0.7048255885866556, 'bert_score': 0.9956709733515074, 'length_difference': 101.14339826839827, 'length_prediction': 255.48593073593074, 'more_comprehensive': 0.6038961038961039}\n",
      "treatment planning\n",
      "{'cosine_similarity': 0.6873182630914403, 'bert_score': 0.9999999814615474, 'length_difference': 95.59055118110236, 'length_prediction': 248.81003937007873, 'more_comprehensive': 0.6771653543307087}\n",
      "professional practice and ethics\n",
      "{'cosine_similarity': 0.7039264191637983, 'bert_score': 0.9999999770751367, 'length_difference': 56.260989010989015, 'length_prediction': 216.33150183150184, 'more_comprehensive': 0.6117216117216118}\n",
      "counseling skills and interventions\n",
      "{'cosine_similarity': 0.6833682030289616, 'bert_score': 0.9999999780351393, 'length_difference': 61.185929648241206, 'length_prediction': 229.15912897822446, 'more_comprehensive': 0.6331658291457286}\n",
      "core counseling attributes\n",
      "{'cosine_similarity': 0.7055947689906411, 'bert_score': 0.9999999585358993, 'length_difference': 64.1413043478261, 'length_prediction': 225.44565217391303, 'more_comprehensive': 0.5652173913043478}\n",
      "#####################################\n",
      "MentaLLaMA-chat-13B\n",
      "intake, assessment, and diagnosis\n",
      "{'cosine_similarity': 0.6376606565255075, 'bert_score': 0.9935064630590992, 'length_difference': -55.59902597402598, 'length_prediction': 98.78463203463204, 'more_comprehensive': 0.14502164502164502}\n",
      "treatment planning\n",
      "{'cosine_similarity': 0.5999674988125898, 'bert_score': 0.9999999633924229, 'length_difference': -59.24212598425197, 'length_prediction': 93.97736220472441, 'more_comprehensive': 0.1889763779527559}\n",
      "professional practice and ethics\n",
      "{'cosine_similarity': 0.5745215682592584, 'bert_score': 0.99999994519866, 'length_difference': -78.56776556776556, 'length_prediction': 81.50274725274726, 'more_comprehensive': 0.1282051282051282}\n",
      "counseling skills and interventions\n",
      "{'cosine_similarity': 0.5685005250879855, 'bert_score': 0.9949748382296794, 'length_difference': -82.86013400335008, 'length_prediction': 85.11306532663316, 'more_comprehensive': 0.16415410385259632}\n",
      "core counseling attributes\n",
      "{'cosine_similarity': 0.48506899862347735, 'bert_score': 0.9999999455783678, 'length_difference': -92.90217391304348, 'length_prediction': 68.40217391304348, 'more_comprehensive': 0.0}\n"
     ]
    }
   ],
   "source": [
    "for model in medical_models_l:\n",
    "    print('#####################################')\n",
    "    try:\n",
    "        print(model)\n",
    "        for idx, cats in enumerate(model_dict[model]):\n",
    "            print(categories[idx])\n",
    "            print(cats)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
