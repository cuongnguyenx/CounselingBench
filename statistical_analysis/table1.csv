setting,Accuracy_Overall,"Accuracy_intake, assessment, and diagnosis",Accuracy_treatment planning,Accuracy_counseling skills and interventions,Accuracy_professional practice and ethics,Accuracy_core counseling attributes
BioMedGPT-LM-7B_zero-shot_None_0,0.4091,0.4361,0.4370000000000001,0.3619,0.4262,0.4869999999999999
Llama3-Med42-70B_zero-shot_None_0,0.6869,0.7079,0.7097000000000001,0.6764,0.6732000000000002,0.5668
Llama3-OpenBioLLM-8B_zero-shot_None_0,0.5626000000000001,0.5574000000000001,0.6159,0.5643,0.5263,0.4429999999999999
MentaLLaMA-chat-13B_zero-shot_None_0,0.4891999999999999,0.4987,0.4924999999999999,0.4675,0.4828,0.4891
Llama3-Med42-8B_zero-shot_None_0,0.6264,0.6247,0.6292,0.6429,0.6262000000000001,0.5690999999999999
Meta-Llama-3-8B-Instruct_zero-shot_None_0,0.6351,0.6488,0.6088,0.655,0.5816,0.6000000000000001
ClinicalCamel-70B_zero-shot_None_0,0.6305999999999999,0.6718000000000002,0.6182,0.6003000000000001,0.5971,0.5563000000000001
Asclepius-7B_zero-shot_None_0,0.2848,0.2508,0.3188999999999999,0.2807999999999999,0.2752,0.4824
Llama-2-7b-hf_zero-shot_None_0,0.4155,0.4428,0.4292999999999999,0.3869999999999999,0.4091999999999999,0.3913
Asclepius-13B_zero-shot_None_0,0.3328,0.2759,0.3729,0.3434,0.3446000000000001,0.4376
Asclepius-Llama3-8B_zero-shot_None_0,0.3475999999999999,0.3354000000000001,0.3607,0.3682,0.3037,0.2994999999999999
meditron-7b_zero-shot_None_0,0.2529,0.2772,0.287,0.243,0.2898,0.1712999999999999
Llama3-OpenBioLLM-70B_zero-shot_None_0,0.6938000000000001,0.7003000000000001,0.7005000000000001,0.7112,0.6964,0.5694999999999999
Llama-2-13b-hf_zero-shot_None_0,0.4513999999999999,0.4725000000000001,0.4650000000000001,0.4495,0.4432,0.565
Llama-2-70b-hf_zero-shot_None_0,0.6063000000000001,0.6236999999999999,0.6296999999999999,0.5927,0.5680000000000001,0.4671
meditron-70b_zero-shot_None_0,0.5665,0.5806000000000001,0.5899,0.5683999999999999,0.5097999999999999,0.3966999999999999
Mistral-7B-Instruct-v0.3_zero-shot_None_0,0.5819000000000001,0.6214,0.5991000000000001,0.5617,0.5793999999999999,0.6559
med42-70b_zero-shot_None_0,0.6865000000000001,0.6731999999999999,0.7003999999999999,0.6972,0.6946999999999999,0.5256
llama-2-7b-chat-hf_zero-shot_None_0,0.4245999999999999,0.4443,0.4819,0.4214,0.4220000000000001,0.2154
Meta-Llama-3-70B-Instruct_zero-shot_None_0,0.7142000000000001,0.7276,0.6878,0.7333999999999999,0.6896,0.6472
medalpaca-13b_zero-shot_None_0,0.525,0.5215000000000001,0.5117,0.5358999999999999,0.513,0.3427
Llama-2-13b-chat-hf_zero-shot_None_0,0.5229,0.5297999999999999,0.5393,0.5122000000000001,0.5284,0.486
Llama-2-70b-chat-hf_zero-shot_None_0,0.6159,0.6581999999999999,0.6199999999999999,0.5968,0.6206,0.4356999999999999
counselingQA-gpt4o_zero-shot_None_0,0.7723,0.8049000000000001,0.7529999999999998,0.7714,0.7415999999999999,0.6103000000000001
medalpaca-7b_zero-shot_None_0,0.4103,0.4150999999999999,0.4355,0.4096,0.4245,0.5221
